# FlexiMart Data Architecture Project

This project demonstrates a comprehensive data architecture solution for FlexiMart, covering traditional relational databases, NoSQL databases, and data warehousing approaches.

## Project Structure

```
studentID-fleximart-data-architecture/
│
├── README.md                           # Root documentation
├── .gitignore                          # Ignore unnecessary files
│
├── data/                               # Input data files (provided)
│   ├── customers_raw.csv
│   ├── products_raw.csv
│   └── sales_raw.csv
│
├── part1-database-etl/
│   ├── README.md                       # Part 1 overview
│   ├── etl_pipeline.py
│   ├── schema_documentation.md
│   ├── business_queries.sql
│   ├── data_quality_report.txt         # Generated by ETL script
│   └── requirements.txt                # Python dependencies
│
├── part2-nosql/
│   ├── README.md                       # Part 2 overview
│   ├── nosql_analysis.md
│   ├── mongodb_operations.js
│   └── products_catalog.json
│
└── part3-datawarehouse/
    ├── README.md                       # Part 3 overview
    ├── star_schema_design.md
    ├── warehouse_schema.sql
    ├── warehouse_data.sql
    └── analytics_queries.sql
```

## Overview

This project is divided into three main parts:

### Part 1: Database ETL
- ETL pipeline implementation
- Database schema design and documentation
- Business intelligence queries
- Data quality reporting

### Part 2: NoSQL
- NoSQL database analysis
- MongoDB operations
- Product catalog in JSON format

### Part 3: Data Warehouse
- Star schema design
- Data warehouse implementation
- Analytics queries for business intelligence

## Getting Started

1. Navigate to each part's directory for specific setup instructions
2. Install dependencies as specified in each part's README
3. Follow the execution steps in each part

## Requirements

- Python 3.8+
- PostgreSQL (for Part 1 and Part 3)
- MongoDB (for Part 2)
- SQL client for running SQL scripts

## Data

Raw data files are provided in the `data/` directory. These files serve as input for the ETL pipeline in Part 1.

